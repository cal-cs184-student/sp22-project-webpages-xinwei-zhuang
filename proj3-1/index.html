<!DOCTYPE html>
<html>
<head>
<title>Page Title</title>

<script src="/js/mathjax/tex-chtml.js" id="MathJax-script" async></script>
<script type="text/javascript" src="LaTeXMathML.js">
  <script defer src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</script>
<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  },
  svg: {
    fontCache: 'global'
  }
};
(function () {
  var script = document.createElement('script');
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js';
  script.async = true;
  document.head.appendChild(script);
})();
</script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>

<script>
function openNav() {
  document.getElementById("mySidebar").style.width = "200px";
  document.getElementById("main").style.marginLeft = "200px";
}

function closeNav() {
  document.getElementById("mySidebar").style.width = "0";
  document.getElementById("main").style.marginLeft= "0";
}
</script>

<style>

code {
  font-family: 'Trebuchet MS', Helvetica, sans-serif;
  color: #336699;
  padding: 2px;
  font-size: 80%;
  line-height: 1.5;
}
.MathJax {
font-size: 1.2em;
}
.center {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
html {
    margin:    0 auto;
    max-width: 1200px;
}
body {
  background-color: RGB(250,250,250);
  color: #303060;
  font-family: 'Trebuchet MS', Helvetica, sans-serif;
box-sizing: border-box;
  width: 100%; 
  padding: 20px;   
  margin: auto;
}
p.big {
  line-height: 1.5;
}
figure figcaption {
    text-align: center;
}
figure {
    display: block;
    margin: 0 1em 1em 0;
    vertical-align: middle;
    text-align: center;
    max-height: 80%;  
    max-width: 80%; 
    top: 0;  
    bottom: 0;  
    left: 0;  
    right: 0;  
    margin-left: auto;
    margin-right: auto ;
}
* {
  box-sizing: border-box;
}
.column {
  float: left;
  width: 50%;
  padding: 5px;
}

/* Clearfix (clear floats) */
.row::after {
  content: "";
  clear: both;
  display: table;
}

.main {
  vertical-align: middle;
  margin-left: 230px; /* Same as the width of the sidenav */
  padding: 0px 10px;
}
/* Clearfix (clear floats) */
.row::after {
  content: "";
  clear: both;
  display: table;
  position: relative;
}
.img {
  display: block;
  max-width: 100%;
  max-height: 100%;
}

.box {
  float: middle;
  width: 80%;
  padding: 20px;
}

.sidenav {
  height: 100%;
  width: 230px;
  position: fixed;
  z-index: 1;
  top: 50;
  left: 0;

  background-color: RGB(250,250,250);
  overflow-x: hidden;
  margin-top: 100px;
  transition: 0.5s;

}

.sidenav a.first {
  padding: 6px 8px 6px 10px;
  text-decoration: none;
  font-size: 20px;
  color: #303060;
  display: block;
  text-align: right;
}

.sidenav a.second {
  padding: 6px 8px 6px 70px;
  text-decoration: none;
  font-size: 15px;
  color: #303060;
  display: block;
  text-align: right;
}

.sidenav a.first:hover {
  font-size: 25px;
  font-weight: bold;
}
.sidenav a.second:hover {
  font-size: 17px;
  font-weight: bold;
}
.sidenav .closebtn {
  position: absolute;
  top: 0;
  right: 25px;
  font-size: 36px;
  margin-left: 50px;
}
.vl {
  border-left: 5px;
  height: 400px;
  width : 1px;
  position: absolute;
  left: 218.5px;
  top:  0 px;
  background-image: linear-gradient( transparent, #303060, transparent);
}
.matrix {
    position: relative;
}

.matrix:before, .matrix:after {
    content: "";
    position: absolute;
    top: 0;
    border: 1px solid #f1f1f1;
    width: 6px;
    height: 100%;
}
.matrix:before {
    left: -6px;
    border-right: 0px;
}
.matrix:after {
    right: -6px;
    border-left: 0px;
}
.matrix: td {
    padding: 5px;    
    text-align: center;
}
table {
  font-family: arial, sans-serif;
  border-collapse: collapse;
  width: 80%;
}

td, th {
  border: 1px solid #dddddd;
  text-align: left;
  padding: 8px;
}

tr:nth-child(even) {
  background-color: #dddddd;
}


}


</style>

</head>
<body>
<div id="mySidebar" class="sidenav">
  <div class="vl"></div>
  <br><br><br>
  <atopic><a class= 'second' href="#1" >Part 1 <span>&#8729;</span></a></atopic>
  <atopic><a class= 'second' href="#2" >Part 2 <span>&#8729;</span></a></atopic>
  <atopic><a class= 'second' href="#3" >Part 3 <span>&#8729;</span></a></atopic>
  <atopic><a class= 'second' href="#4" >Part 4 <span>&#8729;</span></a></atopic>
  <atopic><a class= 'second' href="#5" >Part 5 <span>&#8729;</span></a></atopic>
</div>

<div class="main">
<br><br><br><br>
<h1>Project 3: PathTracer</h1>

<p style="font-size:15px;">CS 284a Computer graphics & Imaging </p> 2022 Spring Kaleab Belete, Xinwei Zhuang</p>
Webpage: <a href="https://cal-cs184-student.github.io/sp22-project-webpages-xinwei-zhuang/proj3-1/index.html"> https://cal-cs184-student.github.io/sp22-project-webpages-xinwei-zhuang/proj3-1/index.html</a>

<hr>

<h2 id="1">Part 1: Ray Generation and Scene Intersection</h2>

<h3>Ray generation and primitive intersection</h3>
Walk through the ray generation and primitive intersection parts of the rendering pipeline.

We firstly map pixels from the unnormalized image space to image space, then map image to camera spac. In the camera space, we calculate the orientation (which is the camera's position) and the direction of the ray. 
    <figure>
  <p><img src="image/1_1.png" style="width:100%">
    <figcaption>ray generation pipeline</figcaption>
</figure>
For each pixel in image space, within a unit square, we use a grid sampler to uniformly sample a counstant number of rays the pixel, and average the sum of the illunimation. The results after generating pixel samples and camera rays are shown below. 
<div class="row">
  <div class="column">
    <figure>
  <p><img src="image/1_2_sky.png" style="width:100%">
    <figcaption>visualization of the direction of camera ray(s) for sky.dae</figcaption>
</figure>
  </div>
  <div class="column">
    <figure>
  <p><img src="image/1_2_banana.png" style="width:100%">
    <figcaption>visualization of the direction of camera ray(s)  for banana.dae</figcaption>
</figure>
  </div>
</div>

<h3>Triangle intersection </h3>
Explain the triangle intersection algorithm you implemented in your own words.
<br><br>
The workflow works as follows:
<ol>
  <li>We check whether the ray is intersected with the plane that triangle lies in using Moller Trumbore Algorithm. The equation to calculate t is optimized to 
  $$\vec{O}+t\vec{D}=(1-b_1-b_2)\vec{P_0}+b_1\vec{P_1}+b_2\vec{P_2}$$
$$\begin{bmatrix}t\\b_1\\b_2 \end{bmatrix} = \frac{1}{\vec{S_1}\vec{E_1}} \begin{bmatrix}\vec{S_2}\vec{E_2}\\ \vec{S_1}\vec{S} \\ \vec{S_2}\vec{D} \end{bmatrix}  $$
Then we compare this t with the far clip and the near clip with the camera. </li>
  <li>If there's a valid intersection, we update t to be the ray's parameter to the intersection point</li>
  <li>We also calculate the surface normal of the intersection point using barycentric coordinates, and we get the BRDF attributes for material. </li>
</ol> 
The CBempty.dae after task 1.3 shows below. 
    <figure>
  <p><img src="image/1_3.png" style="width:100%">
    <figcaption>rendered image  for CBempty.dae</figcaption>
</figure>

<br><br>
<h3>Sphere intersection </h3>
We do the same for the sphere, with a modification for the calculation for t and surface normal.
<figure>
  <p><img src="image/1_4.png" style="width:100%">
    <figcaption>sphere intersection pipelin</figcaption>
</figure>
The CBspheres_lambertian.dae after task 1.4 shows below. 
<figure>
  <p><img src="image/1_4_render.png" style="width:100%">
    <figcaption>rendered image for CBspheres_lambertian.dae</figcaption>
</figure>



<h2 id="2">Part 2: Bounding Volume Hierarchy</h2>
<h3>BVH construction and heuristics</h3>
<b>Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.</b><br><br>

The BVH algorithm is contructed as follows:
<ol>
  <li>We get the bounding box</li>
  <li>Initiate a new BVH node with the bounding box and check the number of primitives</li>
  <li>If we have less primitives than the max_leaf_size, we update the start and end iterator and return the node</li>
  <li>If we have more primitives than the max_leaf_size, 
    <ol>
      <li>Then split the primitives by middle of the longest axis of the bounding box,</li>
      <li>Assign primitives in the children nodes l and r according to its position,</li>
      <li>Chcek whether right/left node has no primitives assigned. If so, assign the leftmost / bottommost primitive in the left child and the rightmost / topmost primitive for the right child to avoid segfault. </li>
       <li>Return the node</li>
    </ol>
   
</ol> 

The heuristics we use for splitting object is to choose the longest axis of the bounding box and split it in the middle. The visualization of the BVH tree of the cow.dae is shown as follows. 
<figure>
  <p><img src="image/bvh tree.png" style="width:100%">
    <figcaption>BVH visualization for cow.dae</figcaption>
</figure>

<h3>normal shading with BVH acceleration</h3>
<b>Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.</b><br><br>
<figure>
  <p><img src="image/3_2_cow.png" style="width:100%">
</figure>
<figure>
  <p><img src="image/3_2_man.png" style="width:100%">
</figure>
<figure>
  <p><img src="image/3_2_luccy.png" style="width:100%">
</figure>

<h3>Comparison</h3>
<b>Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis.</b><br><br>
We rendered the cow.dae with 8 threads rendering. Without BVH it costs 36.2s with 478848 rays traced. The average intersection tests per ray is 734. Generally, using BVH tree will explosively reduce the amount of time needed for complex geometry. 
<table>
  <tr>
    <th> - Cow.dae - </th>
    <th>Without BVH</th>
    <th>With BVH</th>
  </tr>
  <tr>
    <td>time</td>
    <td>36.2s</td>
    <td>0.279s</td>
  </tr>
  <tr>
    <th>- CBlucy.dae - </th>
    <th>Without BVH</th>
    <th>With BVH</th>
  </tr>
  <tr>
    <td>time</td>
    <td>637s</td>
    <td>0.5s</td>
  </tr>
</table>


<h2 id="3">Part 3: Direct Illumination</h2>
<h3>Direct lighting</h3>
<b>Walk through both implementations of the direct lighting function.</b><br><br>
We firstly defined the diffuse BSDF function, both uniformly and with specific pdf. Then we implement zero-bounce illumination, Uniform Hemisphere Sampling and Direct Lighting by Importance Sampling. <br><br>

Direct illumination as the name suggests is when we only consider direct light sources when lighting our scene, we can do this by:
<ol>
<li>Implementing the bidirectional scattering distribution function (BSDF), to represent the colors produced in the outwards direction towards an incoming ray only considering uniform scattering and without negative interaction between incoming and outgoing light, by setting our defuse f function to be reflectance over pi and our sample f function to first updating the *wi to get sample over the pdf from the sample then calling f with wo and *wi.</li>

<li>We can have zero bounce (the light given off by a source) simply equal to the emissions of the intersection bsdf and the one bounce(the light that interacts directly) emulated by either uniform hemisphere sampling or importance sampling based on the given command line input.</li>

<li>Implementing the uniform hemisphere function, which approximates direct lighting by taking uniform samples around a given point over a hemisphere, by creating an offset ray in the direction of the hit point, checking for intersections with that ray and our bvh, and if an intersection is found updating the outwards lighting by multiplying cos theta of the sample with the emissions of the intersection bsdf and the f of the w out and sample all divided by num_sample/2Pi(Monte Carlo). The result is a grainy image as this function gives equal importance to all possible incoming rays.</li>

<li>Implementing Importance sampling which approximates lighting by sampling direct light sources instead of sampling a uniformly over a hemisphere. The basic function this time loops over each light in the scene and calls sample L with the hit point, and pointers for a w input vector and distance and pdf doubles. Then we check if the z axis of (w2o * w_in) are positive with cos theta and create a ray in the direction of the of the hit point with our w input, making sure to update the bounds, and then if there is no intersection with the bvh root and the ray we update the light out in a similar way the the uniform case  using the new sample method and pdf generated from that sample and feeding our (w2o * w_in) to f instead of the sample. This creates a clearer image as the samples are from the things that are most important (direct light sources) and the final picture better approximates a realistic lighting scenario.</li>
</ol>

<figure>
  <p><img src="image/3_1.png" style="width:100%">
    <figcaption>Zero bounce illumination for CBbunny.dae</figcaption>
</figure>

<h3>Rendering</h3>
<b>Images rendered with both implementations of the direct lighting function.</b><br><br>
<h4>Direct Lighting with Uniform Hemisphere Sampling</h4>

<figure>
  <p><img src="image/3_uniform_sampling.png" style="width:100%">
    <figcaption>Uniform Hemisphere Sampling -t 8 -s 16 -l 8</figcaption>
</figure>
<figure>
  <p><img src="image/3_uniform_high.png" style="width:100%">
    <figcaption>Uniform Hemisphere Sampling -t 8 -s 64 -l 32 -m 6</figcaption>
</figure>


<h4>Direct Lighting by Importance Sampling Lights</h4>
<figure>
  <p><img src="image/3_importance.png" style="width:100%">
    <figcaption>Importance Sampling -t 8 -s 1 -l 1 -m 1</figcaption>
</figure>
<figure>
  <p><img src="image/3_importance_high.png" style="width:100%">
    <figcaption>Importance Sampling -t 8 -s 64 -l 32 -m 6</figcaption>
</figure>
<div class="row">
  <div class="column">
    <figure>
  <p><img src="image/3_importance_dragon.png" style="width:100%">
    <figcaption>Importance Sampling -t 8 -s 64 -l 32 -m 6</figcaption>
</figure>
  </div>
  <div class="column">
    <figure>
  <p><img src="image/3_importance_dragon_rate.png" style="width:100%">
    <figcaption>Importance Sampling rate</figcaption>
</figure>
  </div>
</div>


<h3>Lighting sampling rendering</h3>


<b>Focus on one particular scene with at least one area light and compare the noise levels in soft shadows when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, not uniform hemisphere sampling.</b><br><br>

<div class="row">
  <div class="column">
    <figure>
  <p><img src="image/bunny_1_ray.png" style="width:100%">
    <figcaption>1 light rays with 1 sample/pixel</figcaption>
</figure>
  </div>
  <div class="column">
    <figure>
  <p><img src="image/bunny_4_ray.png" style="width:100%">
    <figcaption>4 light rays with 1 sample/pixel</figcaption>
</figure>
  </div>
</div>
<div class="row">
  <div class="column">
    <figure>
  <p><img src="image/bunny_16_ray.png" style="width:100%">
    <figcaption>16 light rays with 1 sample/pixel</figcaption>
</figure>
  </div>
  <div class="column">
    <figure>
  <p><img src="image/bunny_64_ray.png" style="width:100%">
    <figcaption>64 light rays with 1 sample/pixel</figcaption>
</figure>
  </div>
</div>

<h3>Comparison</h3>
<b>Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis.</b><br><br>
<div class="row">
  <div class="column">
    <figure>
  <p><img src="image/3_uniform_high.png" style="width:100%">
    <figcaption>uniform sampling</figcaption>
</figure>
  </div>
  <div class="column">
    <figure>
  <p><img src="image/3_importance_high.png" style="width:100%">
    <figcaption>importance sampling</figcaption>
</figure>
  </div>
</div>
<div class="row">
  <div class="column">
    <figure>
  <p><img src="image/3_uniform_high_rate.png" style="width:100%">
    <figcaption>uniform sampling rate</figcaption>
</figure>
  </div>
  <div class="column">
    <figure>
  <p><img src="image/3_importance_high_rate.png" style="width:100%">
    <figcaption>importance sampling rate</figcaption>
</figure>
  </div>
</div>
As described above uniform hemisphere sampling samples uniformly over an hemisphere for a given point and does not take into consideration the things that are most likely to contribute to the lighting of the scene. Importance sampling samples the important sources (light sources) directly giving the better representation in the final output.Its easy to understand why this changing the method to favor the things that give off the most light would lead to better results. As seen above, Importance sampling is less noisy and it ultimately gives us a better lighting approximation in the direct sampling case.



<h2 id="4">Part 4: Global Illumination </h2>

<h3>Implementation of the indirect lighting </h3>

Global illumination improves on direct illumination by now handling all light including light from secondary sources which only indirectly contribute light to a point in a scene.
To do this we now use an indirect lighting function which we use over of just considering zero bounce + once bounce we no use zero bounce + atleast one bounce.<br><br>

The atleast one bounce function is implemented by 
<ol>
<li>first setting the L out to a single one bounce then while the given rays depth is not at the max ray depth we use Russian roulette with the coin flip function and an arbitrary cpdf probability and use the sample f function giving it the w out and pointers too a vector w in and a pdf. </li>
  <li>After we create an offset ray along the hit point, update its parameters(making sure to increment the depth), check for a bvh intersection with the ray and if there is an intersection we add to L out by recursively calling the function and multiplying the output of the recursive call by the sample and cos theta of the w in vector dividing by pdf*cpdf. </li>
    <li>At the end we return L out and our base case is triggered when we reach max depth(a ray cant bounce anymore). Russian roulette can do a good job at approximating the lighting output and stopping at an appropriate point where the rays are too deep to adequately contribute to the lighting of the scene.</li>
</ol>

<h3>Images rendered with global (direct and indirect) illumination (1024 sample per pixel)</h3>

<figure>
  <p><img src="image/p4/render/BunnyR1024.png" style="width:100%">
    <figcaption>Global illumination for bunny</figcaption>
</figure>

<figure>
  <p><img src="image/p4/render/DragonR1024.png" style="width:100%">
    <figcaption>Global illumination for dragon</figcaption>
</figure>





<h3>Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)</h3>
<div class="row">
  <div class="column">
    <figure>
  <p><img src="image/p4/Direct_vs_Indirect/CBbunny_1024_Direct_M01.png" style="width:100%">
    <figcaption>only direct</figcaption>
</figure>
  </div>
  <div class="column">
    <figure>
  <p><img src="image/p4/Direct_vs_Indirect/CBbunny_Indirect_1024_M2345.png" style="width:100%">
    <figcaption>only indirect</figcaption>
</figure>
  </div>
</div>


<h3>For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.</h3>
<div class="row">
  <div class="column">
    <figure>
  <p><img src="image\p4\s1024Mx\CBbunny_1024_0.png" style="width:100%">
    <figcaption>max ray depth = 0</figcaption>
</figure>
  </div>
  <div class="column">
    <figure>
  <p><img src="image\p4\s1024Mx\CBbunny_1024_1.png" style="width:100%">
    <figcaption>max ray depth = 1</figcaption>
</figure>
  </div>
</div>
<div class="row">
  <div class="column">
    <figure>
  <p><img src="image\p4\s1024Mx\CBbunny_1024_2.png" style="width:100%">
    <figcaption>max ray depth = 2</figcaption>
</figure>
  </div>
  <div class="column">
    <figure>
  <p><img src="image\p4\s1024Mx\CBbunny_1024_3.png" style="width:100%">
    <figcaption>max ray depth = 3</figcaption>
</figure>
  </div>
</div>
<div class="row">
  <div class="column">
    <figure>
  <p><img src="image\p4\s1024Mx\CBbunny_1024_100.png" style="width:100%">
    <figcaption>max ray depth = 100</figcaption>
</figure>
</div>
</div>


<h3>Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.</h3>

<div class="row">
  <div class="column">
    <figure>
  <p><img src="image\p4\l4sx\CBbunny_s1_l4.png" style="width:100%">
    <figcaption>1 sample per pixel</figcaption>
</figure>
  </div>
  <div class="column">
    <figure>
  <p><img src="image\p4\l4sx\CBbunny_s2_l4.png" style="width:100%">
    <figcaption>2 sample per pixel</figcaption>
</figure>
  </div>
</div>
<div class="row">
  <div class="column">
    <figure>
  <p><img src="image\p4\l4sx\CBbunny_s4_l4.png" style="width:100%">
    <figcaption>4 sample per pixel</figcaption>
</figure>
  </div>
  <div class="column">
    <figure>
  <p><img src="image\p4\l4sx\CBbunny_s8_l4.png" style="width:100%">
    <figcaption>8 sample per pixel</figcaption>
</figure>
  </div>
</div>
<div class="row">
  <div class="column">
    <figure>
  <p><img src="image\p4\l4sx\CBbunny_s16_l4.png" style="width:100%">
    <figcaption>16 sample per pixel</figcaption>
</figure>
  </div>
  <div class="column">
    <figure>
  <p><img src="image\p4\l4sx\CBbunny_s64_l4.png" style="width:100%">
    <figcaption>64 sample per pixel</figcaption>
</figure>
  </div>
</div>
<div class="row">
  <div class="column">
    <figure>
  <p><img src="image\p4\l4sx\CBbunny_s1024_l4.png" style="width:100%">
    <figcaption>1024 sample per pixel</figcaption>
</figure>
  </div>
</div>


<h2 id="5">Part 5 Adaptive Sampling</h2>

<h3>Implementation of the adaptive sampling</h3>
We can see that Monte Carlo ray tracing does well in approximating light but it still has has a lot of noise, by selective increasing the sample rate for the given pixels in a scene we can reduce noise while keeping in mind the difficult of a pixel to converge.<br><br>
 
This is done by updating the ray trace pixel function:
<ol>
<li>calculate the sum of the illuminance of each radiance</li>
<li>calculate the sum of the squared illuminance of each radiance</li>
<li>calculate the mean and variance of a batch samples</li>
<li>To reduce the cost, we only perform the convergance on sample batch instead of single pixel. We check if the I (pixel's convergence, based on mean and variable) is converged or not by multiply mean by a max tolerance parameter. If converged, we end the radiance estimation loop. </li>
<li>We update the sample buffer by the sum of radiance divided by the number of sample. </li>
</ol>


<h3>Render</h3>
Pick one scene and render it with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth.

<figure>
  <p><img src="image/p5/bunny_adapt_s2048_a64_0.05_l1_m5_cpdf1.png" style="width:100%">
    <figcaption>adaptive sampling</figcaption>
</figure>

<figure>
  <p><img src="image/p5/bunny_adapt_s2048_a64_0.05_l1_m5_cpdf1_rate.png" style="width:100%">
    <figcaption>adaptive sampling rate</figcaption>
</figure>

</ul>

</div>

</body>
</html>
